{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from collections import Counter # Mode\n",
    "import warnings #Remove Warning Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "path = os.getcwd()\n",
    "all_files = glob.glob(path + \"/dataset/*.txt\")\n",
    "filesnames = os.listdir('dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Track text dataset\n",
    "# It loads txt file dataset. \n",
    "\n",
    "def open_dataset(track_type):\n",
    "    for i in all_files:\n",
    "        if track_type in i:\n",
    "            track_txt = pd.read_csv(i, skiprows = 1, header = None)\n",
    "            track = np.array(track_txt)\n",
    "    return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It converts txt dataset to numpy list.\n",
    "\n",
    "def track_nplist(track_matrix):\n",
    "    full_track =list()\n",
    "    for i in range(len(track_matrix)):\n",
    "        track = list()\n",
    "        for j in track_matrix[i][0]:\n",
    "            track.append(j)\n",
    "        full_track.append(track)\n",
    "    return np.array(full_track, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It finds the position of each type of position. \n",
    "# \"S\": Starting point\n",
    "# \"F\": Finishing point\n",
    "# \"#\": Wall\n",
    "# \"*\": Track list\n",
    "\n",
    "def find_position(track, char):\n",
    "    position_list= []\n",
    "    for i, index in enumerate(track):\n",
    "        for j , character in enumerate(index):\n",
    "            if character ==char:\n",
    "                position_list.append((i,j))\n",
    "    return position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It set up the state of the track list.\n",
    "# It returns the list that exclude wall state.\n",
    "# It also returns the velocity of the car at any given time is limited to -5 to +5.\n",
    "# It also returns the actions state of car. {-1, 0, +1}  \n",
    "\n",
    "def set_up_board_state(track_list):\n",
    "    board_not_wall = []\n",
    "    for row_num, row in enumerate(track_list):\n",
    "        for col_num, col in enumerate(row):\n",
    "            if track_list[row_num][col_num] != '#':\n",
    "                board_not_wall.append([col_num, row_num])\n",
    "    states = []\n",
    "    velocity_car = [i for i in range(-5, 6)]\n",
    "    for loc in board_not_wall:\n",
    "        for x_acc in velocity_car:\n",
    "            for y_acc in velocity_car:\n",
    "                states.append([loc[0], loc[1], x_acc, y_acc])\n",
    "    actions = []\n",
    "    potential_action_value = [-1, 0 , 1]\n",
    "    for i in potential_action_value:\n",
    "        for j in potential_action_value:\n",
    "            actions.append([i, j])\n",
    "    return board_not_wall, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It checks the velocity of the car at any given time is limited to -5 to +5.\n",
    "# If the velocity is below -5, it sets to -5. If the velocity is above +5, it sets to +5\n",
    "\n",
    "def check_velocity_limit(x):\n",
    "    if x>=5:\n",
    "        return 5\n",
    "    elif x<=-5:\n",
    "        return -5\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bresenham Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bresenhamâ€™s Line Generation Algorithm (https://www.geeksforgeeks.org/bresenhams-line-generation-algorithm/)\n",
    "# It generates the line between starting (x1, y1) to (x2, y2).\n",
    "#     Formula\n",
    "#         dx = (x2-x1), dy = (y2-y1)\n",
    "#         m = dx/dy\n",
    "#         y=mx+c\n",
    "\n",
    "def bresenham_algorithm(x1, y1, x2, y2):\n",
    "    path = []\n",
    "    dx = abs(x2-x1)\n",
    "    dy = abs(y2-y1)\n",
    "    slope_x =1 if x1<x2 else slope_x = -1\n",
    "    slope_y =1 if y1<y2 else slope_y = -1\n",
    "    err = dx-dy\n",
    "    while True:\n",
    "        path.append((x1,y1))\n",
    "        if x1 == x2 and y1 == y2:\n",
    "            return path\n",
    "        pk = 2*err\n",
    "        if pk > -dy:\n",
    "            err = err - dy\n",
    "            x1 = x1 + slope_x\n",
    "        if pk < dx:\n",
    "            err = err + dx\n",
    "            y1 = y1 + slope_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform action\n",
    "# It updates the states(x, y, velocity of x, velocity of y)\n",
    "# It computes the possible path from starting (x,y) to (end x, end y) using bresenham algorithm.\n",
    "# It checks whether the possible path contains either finishing points or wall states.\n",
    "# If it is finishing poitns, it returns finishing x, y and sets velocity to 0.\n",
    "# If it is in wall, it checks the crash type. It sets velocity to 0.\n",
    "# If the crash type is soft crash, the position returns nearest position on the track to the place where it crashed.\n",
    "# If the crash type is harsh crash, position is set back to the original starting position\n",
    "# It returns updated x,y, velocity of x, velocity of y.\n",
    "\n",
    "def perform_action(x, y, vel_x, vel_y, finish_locs, wall_locs, track_list, crash_type):\n",
    "    end_x = x + vel_x\n",
    "    end_y = y + vel_y\n",
    "    path = bresenham_algorithm(x, y, end_x, end_y)\n",
    "    finish_detect = False\n",
    "    crash_detect=False\n",
    "    for p in path:\n",
    "        if (p[1], p[0]) in finish_locs:\n",
    "            finish_detect = True\n",
    "            path_point = p\n",
    "            break\n",
    "        elif (p[1], p[0]) in wall_locs:\n",
    "            crash_detect=True\n",
    "            break\n",
    "        path_point = p\n",
    "    if finish_detect:\n",
    "        x, y, vel_x, vel_y = path_point[0], path_point[1], 0, 0\n",
    "    elif crash_detect and crash_type == 'soft_crash':\n",
    "        x, y, vel_x, vel_y = path_point[0], path_point[1], 0, 0\n",
    "    elif crash_detect and crash_type == 'harsh_crash':\n",
    "        random_start = random.choice(start_locs)\n",
    "        x, y, vel_x, vel_y = random_start[1], random_start[0], 0, 0\n",
    "    else:\n",
    "        x+=vel_x\n",
    "        y+=vel_y\n",
    "    return [x, y, vel_x, vel_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "#     It keeps the copy of the v values.\n",
    "#     It iterates all posible states\n",
    "#     It performs actions based on x,y, velocity. \n",
    "#         If it reaches to finishing position, it returns finishing reward\n",
    "#         If it doesn't reach to finishing position, it udpates the reward\n",
    "#     It iterates all possible actions\n",
    "#     It computes the case when acceleration attempt is successful.\n",
    "#          Acceleration = velocity + acceleration\n",
    "#          It checks whether the acceleration is within -5~+5\n",
    "#     It performs actions based on the x,y and updated velocity. \n",
    "#         If it reaches to finishing position, it returns finishing reward\n",
    "#         If it doesn't reach to finishing position, it udpates the reward\n",
    "#     For this assignment, % of successful attempt is 80% , and 20& of unsuccessful attempt\n",
    "#     It computes action reward of the action from successful attempt and unsuccessful attempt, then multiply with learning factor to comptues v value.\n",
    "#     It updates v value if it finds better path. \n",
    "#  After it iterates, it computes for delta (current v value - previous v value)\n",
    "#  If the delta meets error threshold, it breaks.     \n",
    "\n",
    "def value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type):\n",
    "    deltas = []\n",
    "    delta = epsilon + 1\n",
    "    while delta > epsilon:\n",
    "        v_vals_prev = v_vals.copy()\n",
    "        for state_index, state in enumerate(states):\n",
    "            max_reward = -math.inf\n",
    "            x, y, vel_x, vel_y = state[0], state[1], state[2], state[3]\n",
    "            acc_state_unsuccess = perform_action(x, y, vel_x, vel_y, finish_locs, wall_locs,track_list, crash_type) \n",
    "            if (acc_state_unsuccess[1],acc_state_unsuccess[0]) in finish_locs:\n",
    "                unsuccess_reward = 1\n",
    "            else:\n",
    "                for unsuccess_index, unsuccess_state in enumerate(states):\n",
    "                    if unsuccess_state == acc_state_unsuccess:\n",
    "                        unsuccess_reward = v_vals_prev[unsuccess_index]\n",
    "            for a in actions:\n",
    "                acc_x_attempt = vel_x + a[0]\n",
    "                acc_y_attempt = vel_y + a[1]\n",
    "                action_v_x = check_velocity_limit(acc_x_attempt)\n",
    "                action_v_y = check_velocity_limit(acc_y_attempt)\n",
    "                acc_state_success = perform_action(x, y,action_v_x, action_v_y, finish_locs, wall_locs, track_list, crash_type) \n",
    "                if (acc_state_success[1],acc_state_success[0]) in finish_locs:\n",
    "                    success_reward = 1\n",
    "                else:\n",
    "                    for success_index, success_state in enumerate(states):\n",
    "                        if success_state == acc_state_success:\n",
    "                            success_reward = v_vals_prev[success_index]\n",
    "                action_reward = 0.8*success_reward + 0.2*unsuccess_reward\n",
    "                if action_reward > max_reward:\n",
    "                    max_reward = action_reward\n",
    "            if v_vals_prev[state_index] < max_reward *learning_rate:\n",
    "                v_vals[state_index] = max_reward *learning_rate\n",
    "            else:\n",
    "                v_vals[state_index] = v_vals_prev[state_index]   \n",
    "        delta =0\n",
    "        for v_val_index, v_val in enumerate(v_vals):\n",
    "            delta += abs(v_vals[v_val_index] - v_vals_prev[v_val_index])\n",
    "        print(delta)\n",
    "        deltas.append(delta)\n",
    "    return v_vals, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration Testing\n",
    "# This is the algorithm that tests value iteration on track. \n",
    "# It process almost same method as normal value iteration. \n",
    "# For this assignment, % of successful attempt is 80% , and 20& of unsuccessful attempt\n",
    "# It keeps iterationg the states based on the v value that is updated in value iteration algorithm.\n",
    "# If it finds the finishing points, it returns final points, and break the loop.\n",
    "# If it doens't find the finishing points, it moves to next state. \n",
    "\n",
    "def value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type):\n",
    "    reward = -1\n",
    "    starting_points  = random.choice(start_locs)\n",
    "    state =starting_points[1], starting_points[0], 0,0 \n",
    "    max_reward = -math.inf\n",
    "    steps =0\n",
    "    step_list =[]\n",
    "    while reward < 0:\n",
    "        print(state)\n",
    "        step_list.append(state)\n",
    "        x, y, vel_x, vel_y = state[0], state[1], state[2], state[3]\n",
    "        acc_state_unsuccess = perform_action(x, y, vel_x, vel_y, finish_locs, wall_locs, track_list, crash_type) \n",
    "        if (acc_state_unsuccess[1],acc_state_unsuccess[0]) in finish_locs:\n",
    "            unsuccess_reward = 1\n",
    "        else:\n",
    "            for unsuccess_index, unsuccess_state in enumerate(states):\n",
    "                if unsuccess_state == acc_state_unsuccess:\n",
    "                    unsuccess_reward = v_vals[unsuccess_index]\n",
    "        for action in actions:\n",
    "            action_v_x = vel_x +action[0]\n",
    "            action_v_y = vel_y + action[1]\n",
    "            action_v_x = check_velocity_limit(action_v_x)\n",
    "            action_v_y = check_velocity_limit(action_v_y)\n",
    "            acc_state_success = perform_action(x, y,action_v_x, action_v_y, finish_locs, wall_locs, track_list, crash_type) \n",
    "            if (acc_state_success[1],acc_state_success[0]) in finish_locs:\n",
    "                success_reward = 1\n",
    "            else:\n",
    "                for success_index, success_state in enumerate(states):\n",
    "                    if success_state == acc_state_success:\n",
    "                        success_reward = v_vals[success_index]\n",
    "            action_reward = 0.8 * success_reward + 0.2 * unsuccess_reward\n",
    "            if action_reward > max_reward:\n",
    "                max_reward = action_reward\n",
    "                best_action = action\n",
    "        if random.random() <= 0.8:\n",
    "            vel_x = vel_x + best_action[0]\n",
    "            vel_y = vel_y + best_action[1]\n",
    "            vel_x  = check_velocity_limit(vel_x )\n",
    "            vel_y = check_velocity_limit(vel_y)\n",
    "        else:\n",
    "            vel_x = vel_x\n",
    "            vel_y = vel_y\n",
    "        move_state =  perform_action(x, y, vel_x, vel_y, finish_locs, wall_locs, track_list, crash_type)\n",
    "\n",
    "        if (move_state[1], move_state[0]) in finish_locs: \n",
    "            reward = 1\n",
    "            step_list.append(move_state)\n",
    "            break\n",
    "        else:\n",
    "            steps +=1\n",
    "            reward = -1\n",
    "            state = move_state\n",
    "    return steps, step_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon Greedy Policy\n",
    "# It is use to find the optimal balance between exploration and exploitation.\n",
    "# If the random probability is smaller than epsilon, it takes the random action.\n",
    "# If the random probability is larger than epsilon, it takes current best action.\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, q_vals):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0,8)\n",
    "    else:\n",
    "        action = np.argmax(q_vals[state,:])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning\n",
    "#     It randomly select the starting points\n",
    "#     Until it reaches the terminal states\n",
    "#       it choose best action using epsilon greedy policy for given state\n",
    "#       For this assignment, % of successful attempt is 80% , and 20& of unsuccessful attempt\n",
    "#       It performs action and observe reward and new state\n",
    "#       It updates the q value using this formula\n",
    "#          Q(s,a) = Q(s,a)+alpha(r+learning rate maximum action Q(s', a') - Q(s,a))\n",
    "#       It updates the current state.\n",
    "#     It repeats these process until it reaches to maximum number of iteration. \n",
    "\n",
    "def q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes):\n",
    "    step_list =[]\n",
    "    for i in range(total_episodes):\n",
    "        start_pos = random.choice(start_locs)\n",
    "        state = [start_pos[1], start_pos[0], 0, 0]\n",
    "        for state_index_prev, state_prev in enumerate(states):\n",
    "            if state_prev == state:\n",
    "                state_index = state_index_prev\n",
    "        reward = -1\n",
    "        steps = 0\n",
    "        while reward < 0:\n",
    "            a_index = epsilon_greedy_policy(state_index, epsilon, q_vals)\n",
    "            action  = actions[a_index]\n",
    "            if random.uniform(0, 1) <= 0.8:\n",
    "                vel_x = state[2]+action[0]\n",
    "                vel_y = state[3]+action[1]\n",
    "                vel_x_check = check_velocity_limit(vel_x)\n",
    "                vel_y_check = check_velocity_limit(vel_y)\n",
    "            else:\n",
    "                vel_x = state[2]\n",
    "                vel_y = state[3]\n",
    "                vel_x_check = check_velocity_limit(vel_x)\n",
    "                vel_y_check = check_velocity_limit(vel_y)\n",
    "            next_x, next_y, next_vel_x, next_vel_y = perform_action(state[0], state[1], vel_x_check, vel_y_check, finish_locs, wall_locs, track_list,crash_type)\n",
    "            next_state_list = [next_x, next_y, next_vel_x, next_vel_y ]\n",
    "            for state_index_next, state_next in enumerate(states):\n",
    "                if state_next == next_state_list:\n",
    "                    next_state_index = state_index_next\n",
    "            if (next_y, next_x) in finish_locs: \n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "            steps +=1\n",
    "            q_vals[state_index, a_index] = (1-alpha)*q_vals[state_index, a_index]+alpha*(reward+learning_rate*np.max(q_vals[next_state_index]) - q_vals[state_index, a_index])\n",
    "            state = next_state_list\n",
    "            state_index = next_state_index\n",
    "        print('Currnet Episode: ', i)\n",
    "        step_list.append(steps)\n",
    "    return q_vals, step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Test\n",
    "# This is the algorithm that tests Q Learning on track.\n",
    "# Using updated q value, it process almost same method as normal Q Learning.\n",
    "# For this assignment, % of successful attempt is 80% , and 20& of unsuccessful attempt\n",
    "# It performs action state(x,y) and velocity. \n",
    "# If it finds the finishing points, it returns number of steps and break the loop\n",
    "# If it doesn't find the finishing pionts, it moves and updates the state. \n",
    "\n",
    "def q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon):\n",
    "    start_pos = random.choice(start_locs)\n",
    "    state = [start_pos[1], start_pos[0], 0, 0]\n",
    "    for state_index_prev, state_prev in enumerate(states):\n",
    "        if state_prev == state:\n",
    "            state_index = state_index_prev\n",
    "    reward = -1\n",
    "    moves_list = []\n",
    "    moves = 0\n",
    "    while reward < 0:\n",
    "#         print(state)\n",
    "        a = epsilon_greedy_policy(state_index, epsilon, q_vals)\n",
    "        action = actions[a]\n",
    "        if random.uniform(0, 1) <= 0.8:\n",
    "            vel_x = state[2]+action[0]\n",
    "            vel_y = state[3]+action[1]\n",
    "            vel_x_check = check_velocity_limit(vel_x)\n",
    "            vel_y_check = check_velocity_limit(vel_y)\n",
    "        else:\n",
    "            vel_x = state[2]\n",
    "            vel_y = state[3]\n",
    "            vel_x_check = check_velocity_limit(vel_x)\n",
    "            vel_y_check = check_velocity_limit(vel_y)\n",
    "        new_x, new_y, new_vel_x, new_vel_y = perform_action(state[0], state[1], vel_x_check, vel_y_check, finish_locs, wall_locs, track_list, crash_type)\n",
    "        new_state = [new_x, new_y, new_vel_x, new_vel_y]\n",
    "        for state_index_new, state_new in enumerate(states):\n",
    "            if state_new == new_state:\n",
    "                next_new_state_index = state_index_new  \n",
    "        if (new_y, new_x) in finish_locs: \n",
    "            reward = 1\n",
    "            break\n",
    "        else:\n",
    "            reward = -1\n",
    "        moves +=1\n",
    "        state = new_state\n",
    "        state_index = next_new_state_index\n",
    "    return moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA\n",
    "#     It randomly select the starting points\n",
    "#     It choose best action using epsilon greedy policy for given state\n",
    "#     Until it reaches the terminal states\n",
    "#       For this assignment, % of successful attempt is 80% , and 20& of unsuccessful attempt\n",
    "#       It performs action and observe reward and new state\n",
    "#       It updates the q value using this formula\n",
    "#          Q(s,a) = Q(s,a)+alpha(r+learning factor*Q(s', a') - Q(s,a))\n",
    "#       It updates the current state and actions.\n",
    "#     It repeats these process until it reaches to maximum number of iteration. \n",
    "\n",
    "def SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes):\n",
    "    step_list =[]\n",
    "    for i in range(total_episodes):\n",
    "        start_pos = random.choice(start_locs)\n",
    "        state = [start_pos[1], start_pos[0], 0, 0]\n",
    "        for state_index_prev, state_prev in enumerate(states):\n",
    "            if state_prev == state:\n",
    "                state_index = state_index_prev     \n",
    "        a_index = epsilon_greedy_policy(state_index, epsilon, q_vals)\n",
    "        action  = actions[a_index]\n",
    "        reward = -1\n",
    "        steps = 0\n",
    "        while reward < 0:\n",
    "            if random.uniform(0, 1) <= 0.8:\n",
    "                vel_x = state[2]+action[0]\n",
    "                vel_y = state[3]+action[1]\n",
    "                vel_x_check = check_velocity_limit(vel_x)\n",
    "                vel_y_check = check_velocity_limit(vel_y)\n",
    "            else:\n",
    "                vel_x = state[2]\n",
    "                vel_y = state[3]\n",
    "                vel_x_check = check_velocity_limit(vel_x)\n",
    "                vel_y_check = check_velocity_limit(vel_y)\n",
    "            next_x, next_y, next_vel_x, next_vel_y = perform_action(state[0], state[1], vel_x_check, vel_y_check, finish_locs, wall_locs, track_list, crash_type)\n",
    "            next_state_list = [next_x, next_y, next_vel_x, next_vel_y ]\n",
    "            for state_index_next, state_next in enumerate(states):\n",
    "                if state_next == next_state_list:\n",
    "                    next_state_index = state_index_next \n",
    "            if (next_y, next_x) in finish_locs: \n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "            steps +=1\n",
    "            new_a_index = epsilon_greedy_policy(next_state_index, epsilon, q_vals)\n",
    "            new_action  = actions[new_a_index]\n",
    "            q_vals[state_index, a_index] = (1-alpha)*q_vals[state_index, a_index]+alpha*(reward+learning_rate*q_vals[next_state_index, new_a_index] - q_vals[state_index, a_index])\n",
    "            state = next_state_list\n",
    "            state_index = next_state_index\n",
    "            action = new_action\n",
    "            a_index = new_a_index\n",
    "        print('Episode Steps', i)\n",
    "        step_list.append(steps)\n",
    "    return q_vals, step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA Test\n",
    "#     This is the algorithm that tests SARSA on track.\n",
    "#     It randomly select the starting points\n",
    "#     It choose best action using epsilon greedy policy for given state\n",
    "#     Using updated q value, it process almost same method as normal SARSA.\n",
    "#     For this assignment, % of successful attempt is 80% , and 20& of unsuccessful attempt\n",
    "#     It performs action state(x,y) and velocity. \n",
    "#     If it finds the finishing points, it returns number of steps and break the loop\n",
    "#     If it doesn't find the finishing pionts, it moves and updates the state. \n",
    "\n",
    "def SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon):\n",
    "    start_pos = random.choice(start_locs)\n",
    "    state = [start_pos[1], start_pos[0], 0, 0]\n",
    "    for state_index_prev, state_prev in enumerate(states):\n",
    "        if state_prev == state:\n",
    "            state_index = state_index_prev\n",
    "    a_index = epsilon_greedy_policy(state_index, epsilon, q_vals)\n",
    "    action = actions[a_index]\n",
    "    reward = -1\n",
    "    moves_list = []\n",
    "    moves = 0\n",
    "    while reward < 0:\n",
    "#         print(state)\n",
    "        if random.uniform(0, 1) <= 0.8:\n",
    "            vel_x = state[2]+action[0]\n",
    "            vel_y = state[3]+action[1]\n",
    "            vel_x_check = check_velocity_limit(vel_x)\n",
    "            vel_y_check = check_velocity_limit(vel_y)\n",
    "        else:\n",
    "            vel_x = state[2]\n",
    "            vel_y = state[3]\n",
    "            vel_x_check = check_velocity_limit(vel_x)\n",
    "            vel_y_check = check_velocity_limit(vel_y)\n",
    "        new_x, new_y, new_vel_x, new_vel_y = perform_action(state[0], state[1], vel_x_check, vel_y_check, finish_locs, wall_locs, track_list,crash_type)\n",
    "        new_state = [new_x, new_y, new_vel_x, new_vel_y]\n",
    "        for state_index_new, state_new in enumerate(states):\n",
    "            if state_new == new_state:\n",
    "                next_new_state_index = state_index_new    \n",
    "        if (new_y, new_x) in finish_locs: \n",
    "            reward = 1\n",
    "            break\n",
    "        else:\n",
    "            reward = -1\n",
    "        moves +=1\n",
    "        new_a_index = epsilon_greedy_policy(next_new_state_index, epsilon, q_vals)\n",
    "        new_action  = actions[new_a_index]\n",
    "        state = new_state\n",
    "        state_index = next_new_state_index\n",
    "        action = new_action\n",
    "        a_index = new_a_index\n",
    "    return moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration L-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('L-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "v_vals = [0 for i in range(len(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_index, v in enumerate(v_vals):\n",
    "    state_pos = (states[v_index][1], states[v_index][0])\n",
    "    if (state_pos) in finish_locs:\n",
    "        v_vals[v_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_type ='soft_crash'\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.01\n",
    "v_vals, deltas = value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, step_list = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)[0]\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration L-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('L-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "v_vals = [0 for i in range(len(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_index, v in enumerate(v_vals):\n",
    "    state_pos = (states[v_index][1], states[v_index][0])\n",
    "    if (state_pos) in finish_locs:\n",
    "        v_vals[v_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_type ='harsh_crash'\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.01\n",
    "v_vals, deltas = value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, step_list = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)[0]\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration O-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('O-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "v_vals = [0 for i in range(len(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_index, v in enumerate(v_vals):\n",
    "    state_pos = (states[v_index][1], states[v_index][0])\n",
    "    if (state_pos) in finish_locs:\n",
    "        v_vals[v_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_type ='soft_crash'\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.01\n",
    "v_vals, deltas = value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, step_list = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)[0]\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration O-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('O-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "v_vals = [0 for i in range(len(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_index, v in enumerate(v_vals):\n",
    "    state_pos = (states[v_index][1], states[v_index][0])\n",
    "    if (state_pos) in finish_locs:\n",
    "        v_vals[v_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_type ='harsh_crash'\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.01\n",
    "v_vals, deltas = value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, step_list = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)[0]\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration R-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('R-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "v_vals = [0 for i in range(len(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_index, v in enumerate(v_vals):\n",
    "    state_pos = (states[v_index][1], states[v_index][0])\n",
    "    if (state_pos) in finish_locs:\n",
    "        v_vals[v_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_type ='soft_crash'\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.01\n",
    "v_vals, deltas = value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, step_list = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)[0]\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration R-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('R-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "v_vals = [0 for i in range(len(states))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_index, v in enumerate(v_vals):\n",
    "    state_pos = (states[v_index][1], states[v_index][0])\n",
    "    if (state_pos) in finish_locs:\n",
    "        v_vals[v_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_type ='harsh_crash'\n",
    "learning_rate = 0.9\n",
    "epsilon = 0.01\n",
    "v_vals, deltas = value_iteration(states, finish_locs, epsilon, actions, v_vals, finish_locs, wall_locs, track_list, learning_rate, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, step_list = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = value_iteration_testing(start_locs, states, v_vals, finish_locs, wall_locs, track_list, crash_type)[0]\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning L-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('L-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type = 'soft_crash'\n",
    "\n",
    "q_vals, step_list = q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves =q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    #print(total_moves)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', np.mean(total_moves_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning L-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('L-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type = 'harsh_crash'\n",
    "\n",
    "q_vals, step_list = q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves =q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    #print(total_moves)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', np.mean(total_moves_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning O-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('O-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type = 'soft_crash'\n",
    "\n",
    "q_vals, step_list = q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves =q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    #print(total_moves)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', np.mean(total_moves_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning O-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('O-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type = 'harsh_crash'\n",
    "\n",
    "q_vals, step_list = q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves =q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    #print(total_moves)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', np.mean(total_moves_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning R-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('R-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type = 'soft_crash'\n",
    "\n",
    "q_vals, step_list = q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves =q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    #print(total_moves)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', np.mean(total_moves_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning R-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('R-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type = 'harsh_crash'\n",
    "\n",
    "q_vals, step_list = q_learning(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves =q_learning_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    #print(total_moves)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', np.mean(total_moves_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA L-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('L-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type ='soft_crash'\n",
    "\n",
    "q_vals, step_list = SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA L-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('L-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type ='harsh_crash'\n",
    "\n",
    "q_vals, step_list = SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA O-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('O-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type ='soft_crash'\n",
    "\n",
    "q_vals, step_list = SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA O-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('O-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type ='harsh_crash'\n",
    "\n",
    "q_vals, step_list = SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA R-track Soft Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('R-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type ='soft_crash'\n",
    "\n",
    "q_vals, step_list = SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA R-track Harsh Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matrix = open_dataset('R-track')\n",
    "track_list= track_nplist(track_matrix)\n",
    "start_locs = find_position(track_list, 'S')\n",
    "finish_locs = find_position(track_list, 'F')\n",
    "wall_locs = find_position(track_list, '#')\n",
    "board_not_wall, states, actions = set_up_board_state(track_list)\n",
    "q_vals = np.zeros((len(states), len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "total_episodes = 500\n",
    "alpha = 0.1\n",
    "learning_rate = 0.9\n",
    "crash_type ='harsh_crash'\n",
    "\n",
    "q_vals, step_list = SARSA(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type, alpha, learning_rate, epsilon, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_moves_list = []\n",
    "for i in range(10):\n",
    "    total_moves = SARSA_test(states, start_locs, track_list, q_vals, finish_locs, wall_locs, actions, crash_type,epsilon)\n",
    "    total_moves_list.append(total_moves)\n",
    "print('average moves: ', int(np.mean(total_moves_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
